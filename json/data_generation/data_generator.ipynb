{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"paraloq/json_data_extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'topic', 'item_id', 'schema', 'item', 'text', 'medium', '__index_level_0__'],\n",
       "        num_rows: 484\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(\"items\", exist_ok=True)\n",
    "\n",
    "# Extract the \"item\" and \"title\" features from the dataset\n",
    "items = ds[\"train\"][\"item\"]\n",
    "titles = ds[\"train\"][\"title\"]\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Select 10 random items and their corresponding titles\n",
    "random_indices = random.sample(range(len(items)), 100)\n",
    "random_items = [items[i] for i in random_indices]\n",
    "random_titles = [titles[i] for i in random_indices]\n",
    "\n",
    "# Write each item to a separate .json file with formatting\n",
    "for i, (item, title) in enumerate(zip(random_items, random_titles)):\n",
    "    item_dict = json.loads(item)  # Parse the JSON string into a dictionary\n",
    "    sanitized_title = \"\".join(c if c.isalnum() else \"_\" for c in title)  # Sanitize the title for file naming\n",
    "    file_number = f\"{i+1:03}\"  # Format the running number with leading zeros\n",
    "    with open(f\"items/{file_number}_{sanitized_title}.json\", \"w\") as f:\n",
    "        json.dump(item_dict, f, indent=4)  # Write the dictionary as formatted JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import ollama\n",
    "\n",
    "# Directory where the JSON files are stored\n",
    "json_directory = \"items\"\n",
    "\n",
    "# Directory where the generated .md files will be stored (if you want to keep them in the same folder)\n",
    "output_directory = \"items\"\n",
    "\n",
    "# Path to the file where the prompt is stored\n",
    "prompt_file_path = \"prompt_to_generate_instructions.md\"\n",
    "\n",
    "# Function to load the prompt from the file\n",
    "def load_prompt():\n",
    "    with open(prompt_file_path, \"r\") as prompt_file:\n",
    "        return prompt_file.read()\n",
    "\n",
    "# Function to call the local LLM via Ollama\n",
    "def generate_markdown_from_json(json_content, prompt_template):\n",
    "    # Insert the JSON content into the prompt\n",
    "    final_prompt = prompt_template.replace(\"{{json_content}}\", json.dumps(json_content, indent=4))\n",
    "    \n",
    "    # Call the LLM using ollama's chat function\n",
    "    try:\n",
    "        response = ollama.chat(model='gemma2:27b', messages=[{'role': 'user', 'content': final_prompt}])\n",
    "        return response['message']['content']  # Assuming this contains the Markdown content\n",
    "    except ollama.ResponseError as e:\n",
    "        raise Exception(f\"Error while generating markdown: {e.error}\")\n",
    "\n",
    "# Load the prompt from the file\n",
    "prompt_template = load_prompt()\n",
    "\n",
    "# Loop over all JSON files in the directory\n",
    "for filename in os.listdir(json_directory):\n",
    "    if filename.endswith(\".json\"):\n",
    "        json_path = os.path.join(json_directory, filename)\n",
    "        \n",
    "        # Load the JSON content\n",
    "        with open(json_path, \"r\") as json_file:\n",
    "            json_content = json.load(json_file)\n",
    "        \n",
    "        # Generate markdown content using the LLM\n",
    "        markdown_content = generate_markdown_from_json(json_content, prompt_template)\n",
    "        \n",
    "        # Create the output Markdown file\n",
    "        output_filename = f\"{os.path.splitext(filename)[0]}_instruction.md\"\n",
    "        output_path = os.path.join(output_directory, output_filename)\n",
    "        \n",
    "        # Check if the output file already exists\n",
    "        if os.path.exists(output_path):\n",
    "            print(f\"Skipping {output_filename} as it already exists.\")\n",
    "            continue\n",
    "        \n",
    "        # Write the markdown content to the .md file\n",
    "        with open(output_path, \"w\") as md_file:\n",
    "            md_file.write(markdown_content)\n",
    "        \n",
    "        print(f\"Generated {output_filename} from {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import ollama\n",
    "\n",
    "# Directory where the JSON files are stored\n",
    "json_directory = \"items\"\n",
    "\n",
    "# Path to the file where the prompt is stored\n",
    "prompt_file_path = \"prompt_to_generate_solution.md\"\n",
    "\n",
    "# Function to load the prompt from the file\n",
    "def load_prompt():\n",
    "    with open(prompt_file_path, \"r\") as prompt_file:\n",
    "        return prompt_file.read()\n",
    "\n",
    "# Function to load the instruction from the Markdown file\n",
    "def load_instruction(instruction_file_path):\n",
    "    with open(instruction_file_path, \"r\") as instruction_file:\n",
    "        return instruction_file.read()\n",
    "\n",
    "# Function to call the local LLM via Ollama\n",
    "def apply_instructions_to_json(json_content, instruction_content, prompt_template):\n",
    "    # Insert the JSON content and instruction content into the prompt\n",
    "    final_prompt = prompt_template.replace(\"{{json_content}}\", json.dumps(json_content, indent=4))\n",
    "    final_prompt = final_prompt.replace(\"{{instruction_content}}\", instruction_content)\n",
    "\n",
    "    # Call the LLM using Ollama's chat function\n",
    "    try:\n",
    "        response = ollama.chat(model='gemma2:27b', messages=[{'role': 'user', 'content': final_prompt}])\n",
    "        output_text = response['message']['content']\n",
    "\n",
    "        # Attempt to parse the output as JSON\n",
    "        try:\n",
    "            # Remove any code fences or extra text\n",
    "            json_start = output_text.find('{')\n",
    "            json_end = output_text.rfind('}') + 1\n",
    "            json_str = output_text[json_start:json_end]\n",
    "\n",
    "            modified_json = json.loads(json_str)\n",
    "            return modified_json\n",
    "        except json.JSONDecodeError as e:\n",
    "            raise Exception(f\"Failed to parse LLM output as JSON. Error: {e}\\nLLM Output:\\n{output_text}\")\n",
    "    except ollama.ResponseError as e:\n",
    "        raise Exception(f\"Error while applying instructions: {e.error}\")\n",
    "\n",
    "# Load the prompt from the file\n",
    "prompt_template = load_prompt()\n",
    "\n",
    "# Loop over all JSON files in the directory\n",
    "for filename in os.listdir(json_directory):\n",
    "    if filename.endswith(\".json\") and not filename.endswith(\"_solution.json\"):\n",
    "        try:\n",
    "            json_path = os.path.join(json_directory, filename)\n",
    "            base_name = os.path.splitext(filename)[0]\n",
    "            instruction_filename = f\"{base_name}_instruction.md\"\n",
    "            instruction_path = os.path.join(json_directory, instruction_filename)\n",
    "\n",
    "            # Check if the corresponding solution file already exists\n",
    "            solution_filename = f\"{base_name}_solution.json\"\n",
    "            solution_path = os.path.join(json_directory, solution_filename)\n",
    "\n",
    "            if os.path.exists(solution_path):\n",
    "                print(f\"Solution file {solution_filename} already exists for {filename}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Check if the instruction file exists\n",
    "            if not os.path.exists(instruction_path):\n",
    "                print(f\"Instruction file {instruction_filename} not found for {filename}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Load the JSON content\n",
    "            with open(json_path, \"r\") as json_file:\n",
    "                json_content = json.load(json_file)\n",
    "\n",
    "            # Load the instruction content\n",
    "            instruction_content = load_instruction(instruction_path)\n",
    "\n",
    "            # Generate the modified JSON using the LLM\n",
    "            modified_json = apply_instructions_to_json(json_content, instruction_content, prompt_template)\n",
    "\n",
    "            # Create the output JSON file\n",
    "            output_path = os.path.join(json_directory, solution_filename)\n",
    "\n",
    "            # Write the modified JSON content to the .json file\n",
    "            with open(output_path, \"w\") as json_file:\n",
    "                json.dump(modified_json, json_file, indent=4)\n",
    "\n",
    "            print(f\"Generated {solution_filename} by applying instructions to {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing {filename}: {e}\")\n",
    "            continue  # Continue to next iteration\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
